{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b72684a-6f1b-410e-9ddb-1210e0532aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import mean_squared_error, f1_score, roc_auc_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets, linear_model, metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59f8bf29-268c-4904-87f5-8061098ef407",
   "metadata": {},
   "outputs": [],
   "source": [
    "shop = pd.read_csv(\"online_shoppers_intention.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1fd811b-87bf-4ab3-9878-eb6554b9983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shop['Weekend'] = shop['Weekend'].map({False: 0, True: 1})\n",
    "shop['Revenue'] = shop['Revenue'].map({False: 0, True: 1})\n",
    "shop['Month'] = shop['Month'].map({'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6, 'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12})\n",
    "shop['VisitorType'] = shop['VisitorType'].map({'Returning_Visitor': 1, 'New_Visitor': 0})\n",
    "shop.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d3fb053-d9eb-4bec-8f77-dbb8d8ca3458",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = shop['Revenue']\n",
    "shop.drop(['Revenue'], axis = 1, inplace = True)\n",
    "X = shop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2530b66a-4e2f-4e7c-b31d-7d50ee40b0dc",
   "metadata": {},
   "source": [
    "<h1> Decision Tree Classifier </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb39e26b-aa72-4729-a79f-f5705630da81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_decision_tree(X,Y, scoring = True, heatmap = False):\n",
    "    '''\n",
    "    x_train, y_train: training values from the dataset\n",
    "    x_test, y_test: testing values from the dataset to predict on \n",
    "    scoring: Boolean variable to indicate whether to add scoring values\n",
    "    heatmap: Boolean value to indicat whether to plot heatmap\n",
    "    '''\n",
    "    \n",
    "    x_train, x_test,y_train,y_test = train_test_split(X,Y, test_size = 0.2)\n",
    "    \n",
    "    clf = DecisionTreeClassifier()\n",
    "\n",
    "    param_grid = {'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "                  'max_depth': [i for i in range(5)],\n",
    "                  'min_samples_split': [2,5,10,20]}\n",
    "\n",
    "    grid_search = GridSearchCV(clf, param_grid)           \n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    # finding best parameters\n",
    "    new_param = grid_search.best_params_\n",
    "    for item, key in new_param.items():\n",
    "            new_param[item] = [key]\n",
    "\n",
    "    # making predictions\n",
    "    final_clf = GridSearchCV(clf, new_param)\n",
    "    final_clf.fit(x_train, y_train)\n",
    "    predictions = final_clf.predict(x_test)\n",
    "\n",
    "    # storing model and scores\n",
    "    model = \"Decision Tree\"\n",
    "    score_recall = round(recall_score(predictions, y_test),3)\n",
    "    f1_scores = round(f1_score(y_test, predictions),3)\n",
    "    \n",
    "    if heatmap == True:\n",
    "        # heatmap plot to visualize predictions\n",
    "        cm = metrics.confusion_matrix(y_test, predictions)\n",
    "        plt.figure(figsize=(10,10))\n",
    "        sns.heatmap(cm, annot = True, fmt = \".0f\", square = True, cmap = 'icefire');\n",
    "        plt.ylabel('Actual Label');\n",
    "        plt.xlabel('Predicted Label');\n",
    "        all_sample_title = f'{model} \\n F1 {f1_scores} \\n Recall: {score_recall}'\n",
    "        plt.title(all_sample_title, size = 20);\n",
    "        \n",
    "    if scoring == True:\n",
    "        return model, score_recall, f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023400c8-0b18-4b01-9665-e8aaad2bbe2b",
   "metadata": {},
   "source": [
    "<h1> Random Forest Classifier </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "154857c2-3a76-4794-bf2a-8ebe05d11ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random_forest(X,Y, scoring = True, heatmap = False):\n",
    "    '''\n",
    "    x_train, y_train: training values from the dataset\n",
    "    x_test, y_test: testing values from the dataset to predict on \n",
    "    scoring: Boolean variable to indicate whether to add scoring values\n",
    "    heatmap: Boolean value to indicat whether to plot heatmap\n",
    "    '''\n",
    "    \n",
    "    x_train, x_test,y_train,y_test = train_test_split(X,Y, test_size = 0.2)\n",
    "        \n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(x_train, y_train)\n",
    "    predictions = clf.predict(x_test)\n",
    "    cm = metrics.confusion_matrix(y_test, predictions)\n",
    "\n",
    "    # storing models and scores\n",
    "    model = 'Random Forest'\n",
    "    score_recall = round(recall_score(predictions, y_test),3)\n",
    "    f1_scores = round(f1_score(y_test, predictions),3)\n",
    "    \n",
    "    if heatmap == True:\n",
    "        # heatmap plot to visualize predictions\n",
    "        cm = metrics.confusion_matrix(y_test, predictions)\n",
    "        plt.figure(figsize=(10,10))\n",
    "        sns.heatmap(cm, annot = True, fmt = \".0f\", square = True, cmap = 'icefire');\n",
    "        plt.ylabel('Actual Label');\n",
    "        plt.xlabel('Predicted Label');\n",
    "        all_sample_title = f'{model} \\n F1 {f1_scores} \\n Recall: {score_recall}'\n",
    "        plt.title(all_sample_title, size = 20);\n",
    "    \n",
    "    if scoring == True:\n",
    "        return model, score_recall, f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff83086-53cd-45c9-a124-f56b126c9ae1",
   "metadata": {},
   "source": [
    "<h1> Logistic Regression </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64b82cda-fb22-49b2-8da1-ef8f6c2180ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logistic_regr(X,Y, scoring = True, heatmap = False):  \n",
    "    '''\n",
    "    x_train, y_train: training values from the dataset\n",
    "    x_test, y_test: testing values from the dataset to predict on \n",
    "    scoring: Boolean variable to indicate whether to add scoring values\n",
    "    heatmap: Boolean value to indicat whether to plot heatmap\n",
    "    '''\n",
    "    \n",
    "    x_train, x_test,y_train,y_test = train_test_split(X,Y, test_size = 0.2)\n",
    "    \n",
    "    logisticRegr = LogisticRegression()\n",
    "    logisticRegr.fit(x_train, y_train)\n",
    "    predictions = logisticRegr.predict(x_test)\n",
    "\n",
    "    cm = metrics.confusion_matrix(y_test, predictions)\n",
    "\n",
    "    # storing models and scores\n",
    "    model = 'Logistic Regression'\n",
    "    score_recall = round(recall_score(predictions, y_test),3)\n",
    "    f1_scores = round(f1_score(y_test, predictions),3)\n",
    "\n",
    "    if heatmap == True:\n",
    "        # heatmap plot to visualize predictions\n",
    "        cm = metrics.confusion_matrix(y_test, predictions)\n",
    "        plt.figure(figsize=(10,10))\n",
    "        sns.heatmap(cm, annot = True, fmt = \".0f\", square = True, cmap = 'icefire');\n",
    "        plt.ylabel('Actual Label');\n",
    "        plt.xlabel('Predicted Label');\n",
    "        all_sample_title = f'{model} \\n F1 {f1_scores} \\n Recall: {score_recall}'\n",
    "        plt.title(all_sample_title, size = 20);\n",
    "    \n",
    "    if scoring == True:\n",
    "        return model, score_recall, f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3567fae2-0088-4611-8e8d-d93bcec7be79",
   "metadata": {},
   "source": [
    "<h1> KNeighbors Classifier </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34aad4db-6e2c-49ce-a698-b88c243f8b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_knn(X,Y, scoring = True, heatmap = False):\n",
    "    '''\n",
    "    x_train, y_train: training values from the dataset\n",
    "    x_test, y_test: testing values from the dataset to predict on \n",
    "    scoring: Boolean variable to indicate whether to add scoring values\n",
    "    heatmap: Boolean value to indicat whether to plot heatmap\n",
    "    '''\n",
    "    \n",
    "    x_train, x_test,y_train,y_test = train_test_split(X,Y, test_size = 0.2)\n",
    "    \n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(x_train, y_train)\n",
    "    predictions = knn.predict(x_test)\n",
    "    cm = metrics.confusion_matrix(y_test, predictions)\n",
    "\n",
    "    # storing models and scores\n",
    "    model = 'K-Nearest Neighbors'\n",
    "    score_recall = round(recall_score(predictions, y_test),3)\n",
    "    f1_scores = round(f1_score(y_test, predictions),3)\n",
    "\n",
    "    if heatmap == True:\n",
    "        # heatmap plot to visualize predictions\n",
    "        cm = metrics.confusion_matrix(y_test, predictions)\n",
    "        plt.figure(figsize=(10,10))\n",
    "        sns.heatmap(cm, annot = True, fmt = \".0f\", square = True, cmap = 'icefire');\n",
    "        plt.ylabel('Actual Label');\n",
    "        plt.xlabel('Predicted Label');\n",
    "        all_sample_title = f'{model} \\n F1 {f1_scores} \\n Recall: {score_recall}'\n",
    "        plt.title(all_sample_title, size = 20);\n",
    "    \n",
    "    if scoring == True:\n",
    "        return model, score_recall, f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924c9cf9-6d60-4b86-973f-57ab1faa9c93",
   "metadata": {},
   "source": [
    "<h1> Gaussian Naive Bayes </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24f18e37-8534-44d0-9281-b16cd7301994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gnb(X,Y, scoring = True, heatmap = False):\n",
    "    '''\n",
    "    x_train, y_train: training values from the dataset\n",
    "    x_test, y_test: testing values from the dataset to predict on \n",
    "    scoring: Boolean variable to indicate whether to add scoring values\n",
    "    heatmap: Boolean value to indicat whether to plot heatmap\n",
    "    '''\n",
    "    \n",
    "    x_train, x_test,y_train,y_test = train_test_split(X,Y, test_size = 0.2)\n",
    "    \n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(x_train, y_train)\n",
    "    predictions = gnb.predict(x_test)\n",
    "\n",
    "    # storing models and scores\n",
    "    model = 'Gaussian Naive Bayes'\n",
    "    score_recall = round(recall_score(predictions, y_test),3)\n",
    "    f1_scores = round(f1_score(y_test, predictions),3)\n",
    "\n",
    "    if heatmap == True:\n",
    "        # heatmap plot to visualize predictions\n",
    "        cm = metrics.confusion_matrix(y_test, predictions)\n",
    "        plt.figure(figsize=(10,10))\n",
    "        sns.heatmap(cm, annot = True, fmt = \".0f\", square = True, cmap = 'icefire');\n",
    "        plt.ylabel('Actual Label');\n",
    "        plt.xlabel('Predicted Label');\n",
    "        all_sample_title = f'{model} \\n F1 {f1_scores} \\n Recall: {score_recall}'\n",
    "        plt.title(all_sample_title, size = 20);\n",
    "    \n",
    "    if scoring == True:\n",
    "        return model, score_recall, f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e776f81-f01b-47db-b909-10afd33deb80",
   "metadata": {},
   "source": [
    "<h1> Stochastic Gradient Descent </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9db74cb4-ac3d-4941-983a-6f203fa6070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sgd(X,Y, scoring = True, heatmap = False):\n",
    "    '''\n",
    "    x_train, y_train: training values from the dataset\n",
    "    x_test, y_test: testing values from the dataset to predict on \n",
    "    scoring: Boolean variable to indicate whether to add scoring values\n",
    "    heatmap: Boolean value to indicat whether to plot heatmap\n",
    "    '''\n",
    "    \n",
    "    x_train, x_test,y_train,y_test = train_test_split(X,Y, test_size = 0.2)\n",
    "    \n",
    "    clf = SGDClassifier()\n",
    "    clf.fit(x_train, y_train)\n",
    "    predictions = clf.predict(x_test)\n",
    "\n",
    "    # storing models and scores\n",
    "    model = 'Stochastic Gradient Descent'\n",
    "    score_recall = round(recall_score(predictions, y_test),3)\n",
    "    f1_scores = round(f1_score(y_test, predictions),3)\n",
    "\n",
    "    if heatmap == True:\n",
    "        # heatmap plot to visualize predictions\n",
    "        cm = metrics.confusion_matrix(y_test, predictions)\n",
    "        plt.figure(figsize=(10,10))\n",
    "        sns.heatmap(cm, annot = True, fmt = \".0f\", square = True, cmap = 'icefire');\n",
    "        plt.ylabel('Actual Label');\n",
    "        plt.xlabel('Predicted Label');\n",
    "        all_sample_title = f'{model} \\n F1 {f1_scores} \\n Recall: {score_recall}'\n",
    "        plt.title(all_sample_title, size = 20);\n",
    "    \n",
    "    if scoring == True:\n",
    "        return model, score_recall, f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ecf394-369c-4b0a-bab8-c96b1db3ba70",
   "metadata": {},
   "source": [
    "<h1> Support Vector Classification </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52e970e4-c288-44ba-9a78-15206a638f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_svc(X,Y, scoring = True, heatmap = False):\n",
    "    '''\n",
    "    x_train, y_train: training values from the dataset\n",
    "    x_test, y_test: testing values from the dataset to predict on \n",
    "    scoring: Boolean variable to indicate whether to add scoring values\n",
    "    heatmap: Boolean value to indicat whether to plot heatmap\n",
    "    '''\n",
    "    \n",
    "    x_train, x_test,y_train,y_test = train_test_split(X,Y, test_size = 0.2)\n",
    "    \n",
    "    clf = SVC()\n",
    "    clf.fit(x_train, y_train)\n",
    "    predictions = clf.predict(x_test)\n",
    "\n",
    "    # storing models and scores\n",
    "    model = 'Support Vector'\n",
    "    score_recall = round(recall_score(predictions, y_test),3)\n",
    "    f1_scores = round(f1_score(y_test, predictions),3)\n",
    "\n",
    "    if heatmap == True:\n",
    "        # heatmap plot to visualize predictions\n",
    "        cm = metrics.confusion_matrix(y_test, predictions)\n",
    "        plt.figure(figsize=(10,10))\n",
    "        sns.heatmap(cm, annot = True, fmt = \".0f\", square = True, cmap = 'icefire');\n",
    "        plt.ylabel('Actual Label');\n",
    "        plt.xlabel('Predicted Label');\n",
    "        all_sample_title = f'{model} \\n F1 {f1_scores} \\n Recall: {score_recall}'\n",
    "        plt.title(all_sample_title, size = 20);\n",
    "    \n",
    "    if scoring == True:\n",
    "        return model, score_recall, f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c364aa28-e886-4f57-8775-5edc4fa99bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(X,Y):\n",
    "    '''\n",
    "    x_train, y_train: training values from the dataset\n",
    "    x_test, y_test: testing values from the dataset to predict on \n",
    "    '''\n",
    "    models = []\n",
    "    f1s = []\n",
    "    recalls = []\n",
    "    \n",
    "    model_function_names = [run_decision_tree, run_random_forest, run_logistic_regr, \n",
    "                           run_knn, run_gnb, run_sgd, run_svc]\n",
    "    \n",
    "    for name in model_function_names:\n",
    "        model_name, f1, recall = name(X,Y)\n",
    "        models.append(model_name)\n",
    "        f1s.append(f1)\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    scores = {'Classification Model': models, 'F1 Score': f1s, 'Recall Score': recalls}\n",
    "    scores_df = pd.DataFrame.from_dict(scores)\n",
    "    return scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcc46b3b-cce6-4d8c-8548-b5ab500c308d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification Model</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Recall Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.716</td>\n",
       "      <td>0.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gaussian Naive Bayes</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Support Vector</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Classification Model  F1 Score  Recall Score\n",
       "0                Decision Tree     0.718         0.577\n",
       "1                Random Forest     0.724         0.643\n",
       "2          Logistic Regression     0.716         0.497\n",
       "3          K-Nearest Neighbors     0.628         0.386\n",
       "4         Gaussian Naive Bayes     0.494         0.513\n",
       "5  Stochastic Gradient Descent     0.842         0.464\n",
       "6               Support Vector     0.750         0.031"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df = run_all(X,Y)\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e58f58d9-70d4-49b3-be82-7d704766fb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one(model, n, X,Y):\n",
    "\n",
    "    f1s = []\n",
    "    recalls = []\n",
    "    for i in range(n):\n",
    "        model_name, f1, recall = model(X,Y)\n",
    "        f1s.append(f1)\n",
    "        recalls.append(recall)\n",
    "        \n",
    "    scores = {'F1 Score': f1s, 'Recall Score': recalls}\n",
    "    scores_df = pd.DataFrame.from_dict(scores)\n",
    "    \n",
    "    scores_df.style.set_table_attributes(\"style='display:inline'\").set_caption(f'{model} Classification: {n} runs')\n",
    "    return scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4932d3d-bf82-44c7-8722-a36c91911fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Recall Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.709</td>\n",
       "      <td>0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.653</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.670</td>\n",
       "      <td>0.651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.670</td>\n",
       "      <td>0.601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.608</td>\n",
       "      <td>0.589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   F1 Score  Recall Score\n",
       "0     0.709         0.700\n",
       "1     0.653         0.627\n",
       "2     0.670         0.651\n",
       "3     0.670         0.601\n",
       "4     0.608         0.589"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = run_one(run_decision_tree, 5, X,Y)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72c84ac-cc60-4207-96b6-e27507dd3f94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
